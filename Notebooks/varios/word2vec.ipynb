{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ced4f4b",
   "metadata": {},
   "source": [
    "###  1. Importaciones y corpus\n",
    "\n",
    "Importamos PyTorch y utilidades, y definimos un corpus mínimo para demostrar Word2Vec (Skip‑Gram).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e70b4b",
   "metadata": {},
   "source": [
    "####  ¿Qué intenta aprender Word2Vec?\n",
    "\n",
    "Word2Vec entrena un modelo predictivo muy simple: dado un término objetivo `w_t`, intenta asignarle una representación vectorial \\(\\vec{v}_{w_t}\\) capaz de predecir qué palabras pueden aparecer a su alrededor. En la variante Skip-Gram que usamos aquí, maximizamos la probabilidad condicional `P(contexto | palabra_objetivo)` sobre todo el corpus. Al proyectar cada palabra a un espacio continuo de baja dimensión, términos que comparten contexto terminan con vectores cercanos en ese espacio, lo cual captura similitudes semánticas y sintácticas.\n",
    "\n",
    "En la práctica, la red neuronal consta de dos capas lineales: la primera actúa como tabla de embeddings y la segunda proyecta de vuelta al vocabulario. Al entrenar con `CrossEntropyLoss`, la capa de salida aprende una distribución tipo softmax sobre todas las palabras del vocabulario y propaga gradientes hacia la tabla de embeddings, afinando los vectores para cumplir el objetivo predictivo. Este montaje hace que Word2Vec sea ligero pero muy efectivo para aprender representaciones distribuidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa5dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "# torch administra tensores y operaciones en CPU o GPU\n",
    "# nn proporciona bloques reutilizables para construir arquitecturas\n",
    "# optim reúne los optimizadores clásicos (usaremos Adam)\n",
    "# Dataset y DataLoader ordenan el acceso por lotes a los pares (target, contexto)\n",
    "# random es útil para fijar semillas cuando necesitemos reproducibilidad   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d40cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus diminuto para mantener la demostración completamente interpretable\n",
    "corpus = [\n",
    "    \"el gato come pescado\",\n",
    "    \"el perro come carne\",\n",
    "    \"el pájaro vuela alto\",\n",
    "    \"el pez nada en el agua\"\n",
    "]\n",
    "\n",
    "# En un proyecto real se sustituiría por millones de oraciones para captar mejor la estadística del lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee1fce",
   "metadata": {},
   "source": [
    "###  2. Preprocesamiento\n",
    "\n",
    "- Tokenizamos el corpus (separamos palabras)\n",
    "- Construimos el vocabulario\n",
    "- Creamos los mapas palabra↔índice (one‑hot implícito)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce4560",
   "metadata": {},
   "source": [
    "####  ¿Qué logramos con este preprocesamiento?\n",
    "\n",
    "1. **Tokenización:** dividimos cada oración en palabras para conservar el orden y poder deslizar una ventana de contexto.\n",
    "2. **Vocabulario ordenado:** al convertir el conjunto de tokens en una lista ordenada garantizamos índices deterministas para cada palabra.\n",
    "3. **Mapas bidireccionales:** `word_to_idx` y `idx_to_word` nos permiten ir y volver entre representaciones simbólicas y numéricas, requisito para alimentar la red y para interpretar los embeddings al final.\n",
    "4. **Tamaño del vocabulario:** `vocab_size` define cuántas filas tendrá la tabla de embeddings y cuántos logits producirá la capa de salida.\n",
    "\n",
    "Con esta base, cada palabra queda representada como un índice entero que hará las veces de one-hot implícito cuando consultemos la capa `nn.Embedding`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9160b0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: ['agua', 'alto', 'carne', 'come', 'el', 'en', 'gato', 'nada', 'perro', 'pescado', 'pez', 'pájaro', 'vuela']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizamos el corpus\n",
    "tokens = [sentence.split() for sentence in corpus]\n",
    "vocab = sorted(set(sum(tokens, [])))\n",
    "\n",
    "# Mapas palabra <-> índice\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulario:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6178589a",
   "metadata": {},
   "source": [
    "###  3. Crear dataset con ventanas de contexto (Skip‑Gram)\n",
    "\n",
    "Ventana de tamaño 2: para cada palabra objetivo, tomamos hasta 2 palabras antes y 2 después como contexto. Generamos pares (target, context).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed33ed1d",
   "metadata": {},
   "source": [
    "####  Razonamiento del muestreo Skip-Gram\n",
    "\n",
    "- Recorremos cada oración con una ventana simétrica de tamaño `window_size`.\n",
    "- Tratamos cada palabra central como la señal y las palabras vecinas como etiquetas.\n",
    "- Al repetir este proceso en todo el corpus generamos un conjunto explícito de ejemplos `(palabra_objetivo, palabra_contexto)` listos para aprendizaje supervisado.\n",
    "- La densidad de ejemplos depende del tamaño de la ventana: ventanas grandes capturan relaciones temáticas, ventanas pequeñas capturan relaciones sintácticas.\n",
    "\n",
    "Este muestreo es equivalente a maximizar la suma de log-probabilidades `\\sum_t \\sum_{c \\in C_t} \\log P(c | w_t)`, donde `C_t` es el conjunto de contextos válidos alrededor de `w_t`. Así entrenamos a la red para que cada vector contenga información predictiva sobre su vecindario lingüístico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d344f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de pares (target, context): [('el', 'gato'), ('el', 'come'), ('gato', 'el'), ('gato', 'come'), ('gato', 'pescado'), ('come', 'el'), ('come', 'gato'), ('come', 'pescado'), ('pescado', 'gato'), ('pescado', 'come')]\n"
     ]
    }
   ],
   "source": [
    "def generate_skipgram_pairs(tokens, window_size=2):\n",
    "    \"\"\"Genera pares (palabra_objetivo, palabra_contexto) recorriendo ventanas Skip-Gram.\"\"\"\n",
    "    pairs = []\n",
    "    for sentence in tokens:\n",
    "        for i, target in enumerate(sentence):\n",
    "            for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    # Excluimos la palabra central y añadimos únicamente los vecinos válidos\n",
    "                    pairs.append((target, sentence[j]))\n",
    "    return pairs\n",
    "\n",
    "pairs = generate_skipgram_pairs(tokens, window_size=2)\n",
    "print(\"Ejemplo de pares (target, context):\", pairs[:10])\n",
    "# Observa cómo cada palabra aparece tantas veces como ventanas la incluyan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c09c843",
   "metadata": {},
   "source": [
    "###  4. Dataset en PyTorch\n",
    "\n",
    "Empaquetamos los pares (target, context) en un `Dataset` y un `DataLoader` para entrenamiento por lotes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69d99902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de pares (target, context): [('el', 'gato'), ('el', 'come'), ('gato', 'el'), ('gato', 'come'), ('gato', 'pescado'), ('come', 'el'), ('come', 'gato'), ('come', 'pescado'), ('pescado', 'gato'), ('pescado', 'come')]\n"
     ]
    }
   ],
   "source": [
    "def generate_skipgram_pairs(tokens, window_size=2):\n",
    "    pairs = []\n",
    "    for sentence in tokens:\n",
    "        for i, target in enumerate(sentence):\n",
    "            for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    pairs.append((target, sentence[j]))\n",
    "    return pairs\n",
    "\n",
    "pairs = generate_skipgram_pairs(tokens, window_size=2)\n",
    "print(\"Ejemplo de pares (target, context):\", pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f5a3be",
   "metadata": {},
   "source": [
    "###  5. Modelo Word2Vec (Skip‑Gram)\n",
    "\n",
    "Arquitectura mínima:\n",
    "- Capa de embeddings (convierte índices→vectores)\n",
    "- Capa lineal de salida (vocabulario) con softmax implícito en la pérdida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb77d421",
   "metadata": {},
   "source": [
    "####  Desglose de la arquitectura\n",
    "\n",
    "- `nn.Embedding(vocab_size, embedding_dim)` es conceptualmente una matriz `W_in` de tamaño `(vocab_size, embedding_dim)`; cada fila es el vector asociado a una palabra.\n",
    "- La capa lineal `nn.Linear(embedding_dim, vocab_size)` actúa como `W_out`, que proyecta el embedding al espacio del vocabulario para producir un logit por palabra.\n",
    "- Durante el *forward*, seleccionamos la fila correspondiente a cada palabra del lote, la multiplicamos por `W_out` y obtenemos una distribución sin normalizar.\n",
    "- La pérdida `CrossEntropyLoss` aplica softmax + log-loss, forzando que el logit de la palabra de contexto correcta sea mayor que los demás.\n",
    "\n",
    "Así, los embeddings se mueven en la dirección que maximiza la probabilidad de observar vecinos reales y minimiza la de vecinos incorrectos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c9d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataset(Dataset):\n",
    "    \"\"\"Envuelve los pares Skip-Gram y los expone como tensores listos para PyTorch.\"\"\"\n",
    "\n",
    "    def __init__(self, pairs, word_to_idx):\n",
    "        self.pairs = pairs\n",
    "        self.word_to_idx = word_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target, context = self.pairs[idx]\n",
    "        target_idx = torch.tensor(self.word_to_idx[target], dtype=torch.long)\n",
    "        context_idx = torch.tensor(self.word_to_idx[context], dtype=torch.long)\n",
    "        return target_idx, context_idx\n",
    "\n",
    "dataset = Word2VecDataset(pairs, word_to_idx)\n",
    "# DataLoader baraja los pares en cada epoch y agrupa 8 ejemplos por lote\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c6e66",
   "metadata": {},
   "source": [
    "### ⚙️ 6. Entrenamiento\n",
    "\n",
    "Optimizamos con `CrossEntropyLoss` y `Adam`. El objetivo es predecir la palabra de contexto dada la palabra objetivo.\n",
    "\n",
    "**Ciclo de entrenamiento simplificado:**\n",
    "1. Obtenemos embeddings para cada palabra del lote.\n",
    "2. Calculamos logits para todas las palabras del vocabulario y derivamos una distribución softmax implícita.\n",
    "3. Evaluamos la entropía cruzada entre dicha distribución y el índice real del contexto.\n",
    "4. Retropropagamos los gradientes para ajustar tanto la tabla de embeddings como la capa de salida.\n",
    "\n",
    "Aunque este ejemplo usa *full softmax*, en corpora grandes se acostumbra a combinarlo con *negative sampling* o *hierarchical softmax* para reducir el costo computacional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce8ce13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecModel(nn.Module):\n",
    "    \"\"\"Versión mínima del modelo Skip-Gram con embeddings aprendibles.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, target_word):\n",
    "        emb = self.embeddings(target_word)  # Recuperamos los vectores de cada palabra del lote\n",
    "        logits = self.output(emb)  # Proyectamos a vocab_size logits para evaluar cada posible contexto\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d8aa0",
   "metadata": {},
   "source": [
    "###  7. Ver los embeddings finales\n",
    "\n",
    "Inspeccionamos la matriz de embeddings (una fila por palabra). Palabras con contextos similares tendrán vectores cercanos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52f42f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/200, Loss: 9.1628\n",
      "Epoch 80/200, Loss: 8.4335\n",
      "Epoch 120/200, Loss: 8.3221\n",
      "Epoch 160/200, Loss: 8.2862\n",
      "Epoch 200/200, Loss: 8.2487\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 8  # Longitud de cada vector semántico\n",
    "model = Word2VecModel(vocab_size, embedding_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Softmax + log-loss en un paso estable\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 200\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for target, context in dataloader:\n",
    "        optimizer.zero_grad()  # Limpiamos gradientes acumulados\n",
    "        logits = model(target)\n",
    "        loss = criterion(logits, context)\n",
    "        loss.backward()  # Retropropagamos el error hacia las dos matrices de pesos\n",
    "        optimizer.step()  # Actualizamos W_in y W_out simultáneamente\n",
    "        total_loss += loss.item()\n",
    "    if (epoch + 1) % 40 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ec3ee",
   "metadata": {},
   "source": [
    "###  En General\n",
    "\n",
    "- La ventana de contexto se desplaza palabra por palabra.\n",
    "- Generamos pares (target, contexto) para aprendizaje supervisado.\n",
    "- El modelo aprende a predecir el contexto a partir de la palabra central.\n",
    "- Los pesos de la capa de embeddings son las representaciones vectoriales (significado) de las palabras.\n",
    "- Podemos medir similitud semántica con métricas como coseno o distancia euclidiana sobre esos vectores.\n",
    "- Al promediar embeddings de frases podemos construir características para modelos posteriores.\n",
    "\n",
    "Una vez entrenado, basta con congelar `model.embeddings` y reutilizarlo como extractor de características en tareas de clasificación, clustering o visualización (p.ej. reduciendo de 8D a 2D con UMAP o t-SNE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d585d5",
   "metadata": {},
   "source": [
    "####  ¿Cómo interpretar la matriz de embeddings?\n",
    "\n",
    "- Cada fila corresponde a una palabra en el orden definido por `word_to_idx`.\n",
    "- Componentes positivos/negativos indican afinidades latentes aprendidas durante el entrenamiento.\n",
    "- Para comparar palabras, calcula el coseno entre dos filas (`torch.nn.functional.cosine_similarity`).\n",
    "- También es posible proyectar la matriz a 2D y visualizar clusters para validar si animales, acciones, etc. quedan agrupados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6873981e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agua       -> [-0.2564436197280884, 1.5151938199996948, -0.3949437141418457, -2.2353787422180176, -1.5946879386901855, -1.170777678489685, -1.455666184425354, 0.8406234383583069]\n",
      "alto       -> [-0.23509903252124786, -1.768825650215149, 0.650020956993103, 1.8077776432037354, 0.2962290346622467, -1.962864637374878, 0.3335600197315216, 2.5872435569763184]\n",
      "carne      -> [2.601318597793579, 0.222939595580101, 1.8648544549942017, 0.34594205021858215, 1.909814476966858, 0.4502359926700592, 0.20215937495231628, -1.1665366888046265]\n",
      "come       -> [0.5047829151153564, 2.1917102336883545, 0.21772634983062744, -2.2317585945129395, 0.5539979934692383, 1.4984265565872192, -1.1155365705490112, -0.2711646854877472]\n",
      "el         -> [-1.2901389598846436, -0.9246841073036194, 2.1643381118774414, -0.2278233915567398, 0.07226025313138962, -0.40380069613456726, 1.1552389860153198, -0.1157672330737114]\n",
      "en         -> [-0.15550340712070465, -1.967090129852295, -3.3128697872161865, 0.16994978487491608, -1.3420979976654053, 1.7008541822433472, -0.6491376757621765, -0.5068223476409912]\n",
      "gato       -> [-1.8396319150924683, 2.585141181945801, 0.6936855316162109, 0.6729217767715454, 0.8237004280090332, 0.42855575680732727, -3.241218090057373, 1.3651912212371826]\n",
      "nada       -> [1.8059862852096558, -0.9760870337486267, -0.43887457251548767, -1.7244850397109985, -2.247591495513916, -0.6316505074501038, -0.4300194978713989, 1.2142733335494995]\n",
      "perro      -> [2.2882466316223145, 1.4112393856048584, -0.06708871573209763, 1.1242440938949585, -1.8562248945236206, 2.012209892272949, -2.173090696334839, 1.4116780757904053]\n",
      "pescado    -> [0.5681679248809814, 1.4038785696029663, 0.6581540107727051, 0.7883162498474121, -1.2288563251495361, 3.2216787338256836, 1.837476372718811, -0.1961681991815567]\n",
      "pez        -> [-0.8964190483093262, -0.33873990178108215, -2.6267735958099365, -2.4410946369171143, -1.4812822341918945, -0.35479915142059326, 1.3107770681381226, 1.1202282905578613]\n",
      "pájaro     -> [-0.9873871803283691, -0.8592395782470703, -2.197237253189087, 0.4101896286010742, 2.5915184020996094, -0.16935773193836212, -1.2617266178131104, 2.264720916748047]\n",
      "vuela      -> [1.4857443571090698, 1.5219413042068481, -1.262838363647461, -0.0200213436037302, 1.2840948104858398, -0.6041547656059265, 1.3734796047210693, 3.1761012077331543]\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.embeddings.weight.data  # Tensores con las coordenadas finales\n",
    "for word, idx in word_to_idx.items():\n",
    "    print(f\"{word:10s} -> {embeddings[idx].tolist()}\")\n",
    "\n",
    "# Puedes almacenar esta matriz para cálculos posteriores o para visualizarla con herramientas como TensorBoard Projector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redesneu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
