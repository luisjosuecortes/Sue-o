{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ced4f4b",
   "metadata": {},
   "source": [
    "###  1. Importaciones y corpus\n",
    "\n",
    "Importamos PyTorch y utilidades, y definimos un corpus mínimo para demostrar Word2Vec (Skip‑Gram).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa5dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d40cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"el gato come pescado\",\n",
    "    \"el perro come carne\",\n",
    "    \"el pájaro vuela alto\",\n",
    "    \"el pez nada en el agua\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee1fce",
   "metadata": {},
   "source": [
    "###  2. Preprocesamiento\n",
    "\n",
    "- Tokenizamos el corpus (separamos palabras)\n",
    "- Construimos el vocabulario\n",
    "- Creamos los mapas palabra↔índice (one‑hot implícito)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9160b0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: ['agua', 'alto', 'carne', 'come', 'el', 'en', 'gato', 'nada', 'perro', 'pescado', 'pez', 'pájaro', 'vuela']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizamos el corpus\n",
    "tokens = [sentence.split() for sentence in corpus]\n",
    "vocab = sorted(set(sum(tokens, [])))\n",
    "\n",
    "# Mapas palabra <-> índice\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulario:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6178589a",
   "metadata": {},
   "source": [
    "###  3. Crear dataset con ventanas de contexto (Skip‑Gram)\n",
    "\n",
    "Ventana de tamaño 2: para cada palabra objetivo, tomamos hasta 2 palabras antes y 2 después como contexto. Generamos pares (target, context).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d344f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de pares (target, context): [('el', 'gato'), ('el', 'come'), ('gato', 'el'), ('gato', 'come'), ('gato', 'pescado'), ('come', 'el'), ('come', 'gato'), ('come', 'pescado'), ('pescado', 'gato'), ('pescado', 'come')]\n"
     ]
    }
   ],
   "source": [
    "def generate_skipgram_pairs(tokens, window_size=2):\n",
    "    pairs = []\n",
    "    for sentence in tokens:\n",
    "        for i, target in enumerate(sentence):\n",
    "            for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    pairs.append((target, sentence[j]))\n",
    "    return pairs\n",
    "\n",
    "pairs = generate_skipgram_pairs(tokens, window_size=2)\n",
    "print(\"Ejemplo de pares (target, context):\", pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c09c843",
   "metadata": {},
   "source": [
    "###  4. Dataset en PyTorch\n",
    "\n",
    "Empaquetamos los pares (target, context) en un `Dataset` y un `DataLoader` para entrenamiento por lotes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69d99902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de pares (target, context): [('el', 'gato'), ('el', 'come'), ('gato', 'el'), ('gato', 'come'), ('gato', 'pescado'), ('come', 'el'), ('come', 'gato'), ('come', 'pescado'), ('pescado', 'gato'), ('pescado', 'come')]\n"
     ]
    }
   ],
   "source": [
    "def generate_skipgram_pairs(tokens, window_size=2):\n",
    "    pairs = []\n",
    "    for sentence in tokens:\n",
    "        for i, target in enumerate(sentence):\n",
    "            for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    pairs.append((target, sentence[j]))\n",
    "    return pairs\n",
    "\n",
    "pairs = generate_skipgram_pairs(tokens, window_size=2)\n",
    "print(\"Ejemplo de pares (target, context):\", pairs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f5a3be",
   "metadata": {},
   "source": [
    "###  5. Modelo Word2Vec (Skip‑Gram)\n",
    "\n",
    "Arquitectura mínima:\n",
    "- Capa de embeddings (convierte índices→vectores)\n",
    "- Capa lineal de salida (vocabulario) con softmax implícito en la pérdida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c9d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, pairs, word_to_idx):\n",
    "        self.pairs = pairs\n",
    "        self.word_to_idx = word_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target, context = self.pairs[idx]\n",
    "        return torch.tensor(self.word_to_idx[target]), torch.tensor(self.word_to_idx[context])\n",
    "\n",
    "dataset = Word2VecDataset(pairs, word_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c6e66",
   "metadata": {},
   "source": [
    "### ⚙️ 6. Entrenamiento\n",
    "\n",
    "Optimizamos con `CrossEntropyLoss` y `Adam`. El objetivo es predecir la palabra de contexto dada la palabra objetivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce8ce13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, target_word):\n",
    "        emb = self.embeddings(target_word)\n",
    "        out = self.output(emb)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d8aa0",
   "metadata": {},
   "source": [
    "###  7. Ver los embeddings finales\n",
    "\n",
    "Inspeccionamos la matriz de embeddings (una fila por palabra). Palabras con contextos similares tendrán vectores cercanos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52f42f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/200, Loss: 9.1965\n",
      "Epoch 80/200, Loss: 8.4766\n",
      "Epoch 120/200, Loss: 8.3430\n",
      "Epoch 160/200, Loss: 8.2753\n",
      "Epoch 200/200, Loss: 8.2674\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 8\n",
    "model = Word2VecModel(vocab_size, embedding_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for target, context in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(target)\n",
    "        loss = criterion(output, context)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch + 1) % 40 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ec3ee",
   "metadata": {},
   "source": [
    "###  En General\n",
    "\n",
    "- La ventana de contexto se desplaza palabra por palabra.\n",
    "- Generamos pares (target, contexto) para aprendizaje supervisado.\n",
    "- El modelo aprende a predecir el contexto a partir de la palabra central.\n",
    "- Los pesos de la capa de embeddings son las representaciones vectoriales (significado) de las palabras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6873981e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agua       -> [0.35946959257125854, 0.23149815201759338, 1.0404404401779175, -0.2307528406381607, -1.9380111694335938, -1.3543272018432617, 2.47517466545105, 2.420862913131714]\n",
      "alto       -> [1.2128000259399414, -2.3942513465881348, -0.301423043012619, -1.10421884059906, 2.416069984436035, 1.3163093328475952, -0.593668520450592, 2.4861366748809814]\n",
      "carne      -> [-3.0026299953460693, 2.461963415145874, 0.3777582347393036, -2.414140462875366, 1.2446975708007812, -0.6849075555801392, -0.4919763505458832, -0.2393103688955307]\n",
      "come       -> [-2.616908550262451, 0.35495927929878235, -0.15568451583385468, -0.6339329481124878, -1.7604267597198486, 1.0748422145843506, 2.94465970993042, -1.3403608798980713]\n",
      "el         -> [-0.2979934513568878, -2.603065013885498, 1.8233269453048706, 0.16880477964878082, 2.2370550632476807, -1.4897934198379517, 0.5872586965560913, -0.6455591320991516]\n",
      "en         -> [-0.4081358015537262, -0.3527601957321167, -0.39999285340309143, 2.420285940170288, -1.4807320833206177, -2.372097969055176, -1.5130773782730103, -0.6444902420043945]\n",
      "gato       -> [0.187005415558815, 1.8047728538513184, -2.228001117706299, -2.4496009349823, -0.7304267883300781, -1.3866357803344727, 0.36110660433769226, -1.2429935932159424]\n",
      "nada       -> [-1.698655605316162, -0.25270769000053406, 0.3664093017578125, 2.0694987773895264, -1.6466116905212402, -0.9090902805328369, 0.7484950423240662, 2.936408281326294]\n",
      "perro      -> [-2.9129955768585205, -0.9126837253570557, -2.9020438194274902, -1.3034745454788208, -0.594936192035675, -0.042576223611831665, 0.09777268022298813, -0.41079971194267273]\n",
      "pescado    -> [-1.8103779554367065, 0.0819958969950676, -1.4582661390304565, 1.390702724456787, 2.8653886318206787, 0.44701266288757324, 0.4750215411186218, -2.7283616065979004]\n",
      "pez        -> [3.643855571746826, -1.2684298753738403, 0.9598594307899475, 0.4943724572658539, -2.7032833099365234, -1.7670363187789917, 0.2903212308883667, 0.05002583935856819]\n",
      "pájaro     -> [1.324052095413208, -0.43274569511413574, -2.731863498687744, 2.0724220275878906, 1.3601717948913574, -1.0701674222946167, 0.4507412016391754, 2.2162256240844727]\n",
      "vuela      -> [2.875998020172119, -0.31088876724243164, -1.5187675952911377, -0.9791653752326965, 0.12437812238931656, 0.7314828634262085, 2.374873399734497, 2.492445707321167]\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.embeddings.weight.data\n",
    "for word, idx in word_to_idx.items():\n",
    "    print(f\"{word:10s} -> {embeddings[idx].tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redesneu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
